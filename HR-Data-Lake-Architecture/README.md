# üèóÔ∏è HR Data Lake Architecture
> **Dise√±o Conceptual de Arquitectura de Datos para Gesti√≥n del Talento**

[![Data Engineering](https://img.shields.io/badge/Data-Engineering-blue)](https://github.com/JameLalupu)
[![People Analytics](https://img.shields.io/badge/People-Analytics-green)](https://github.com/JameLalupu)
[![Python](https://img.shields.io/badge/Python-3.9+-yellow)](https://www.python.org/)

---

## üìã Tabla de Contenidos
- [Contexto del Problema](#contexto-del-problema)
- [Arquitectura Propuesta](#arquitectura-propuesta)
- [Capas del Data Lake](#capas-del-data-lake)
- [Flujo ETL Detallado](#flujo-etl-detallado)
- [Data Governance](#data-governance)
- [Stack Tecnol√≥gico](#stack-tecnol√≥gico)
- [Casos de Uso](#casos-de-uso)

---

## üéØ Contexto del Problema

### El Desaf√≠o
Las organizaciones modernas gestionan datos de RR.HH. desde m√∫ltiples sistemas:
- **SAP SuccessFactors** (n√≥minas, headcount)
- **Workday** (compensaciones, beneficios)
- **ADP** (asistencia, tiempo)
- **Sistemas locales** (evaluaciones, encuestas)

**Problema**: Datos fragmentados ‚Üí Decisiones lentas e inconsistentes.

### La Soluci√≥n: Data Lake Centralizado
Un repositorio √∫nico que:
1. ‚úÖ Centraliza todas las fuentes de datos
2. ‚úÖ Mantiene historial completo (auditor√≠a)
3. ‚úÖ Permite an√°lisis predictivo (ML)
4. ‚úÖ Garantiza calidad y governance

---

## üèõÔ∏è Arquitectura Propuesta

Esta arquitectura sigue el patr√≥n **medallion** con tres capas: **Staging ‚Üí Raw ‚Üí Master**

```mermaid
graph TB
    subgraph Sources["üì• Fuentes de Datos"]
        SAP[SAP SuccessFactors]
        WD[Workday]
        ADP[ADP]
        LOCAL[Sistemas Locales]
    end
    
    subgraph Staging["üö™ STAGING LAYER - Zona de Aterrizaje"]
        CSV1[nominas_20260204.csv]
        CSV2[asistencias_20260204.csv]
        JSON1[evaluaciones.json]
    end
    
    subgraph Raw["üì¶ RAW LAYER - Almacenamiento Hist√≥rico"]
        AVRO1[nominas/year=2026/month=02/data.avro]
        AVRO2[asistencias/year=2026/month=02/data.avro]
        AVRO3[evaluaciones/year=2026/month=02/data.avro]
    end
    
    subgraph Master["‚≠ê MASTER LAYER - Zona Anal√≠tica"]
        PARQ1[employee_metrics.parquet]
        PARQ2[headcount_history.parquet]
        PARQ3[turnover_indicators.parquet]
    end
    
    subgraph Consumo["üìä Consumo"]
        PBI[Power BI Dashboards]
        JUP[Jupyter Notebooks]
        API[REST APIs]
    end
    
    SAP --> CSV1
    WD --> CSV2
    LOCAL --> JSON1
    
    CSV1 --> |ETL Python/Spark| AVRO1
    CSV2 --> |Validaci√≥n + Conversi√≥n| AVRO2
    JSON1 --> |Schema Evolution| AVRO3
    
    AVRO1 --> |Agregaci√≥n + Joins| PARQ1
    AVRO2 --> |C√°lculo de KPIs| PARQ2
    AVRO3 --> |Feature Engineering| PARQ3
    
    PARQ1 --> PBI
    PARQ2 --> JUP
    PARQ3 --> API
```

---

## üîÑ Capas del Data Lake

### 1Ô∏è‚É£ STAGING LAYER - Zona de Aterrizaje

**Prop√≥sito**: Recepci√≥n de datos crudos **sin transformaci√≥n**.

| Caracter√≠stica | Detalle |
|----------------|---------|
| **Formato** | CSV, JSON, XML (tal como llega de la fuente) |
| **Esquema** | No validado (raw) |
| **Retenci√≥n** | 7 d√≠as (luego se archiva o elimina) |
| **Particionamiento** | Por fecha de ingesta (`/staging/YYYY-MM-DD/`) |

**Ejemplo de archivo**:
```csv
# staging/nominas/2026-02-04/export_sap.csv
employee_id,nombre,salario_bruto,fecha_pago,departamento
10234,Juan P√©rez,5500.00,2026-02-01,Ventas
10567,Mar√≠a Garc√≠a,7200.00,2026-02-01,TI
```

**‚ö†Ô∏è Sin validaciones**: Si SAP env√≠a un campo corrupto, se almacena igual (trazabilidad).

---

### 2Ô∏è‚É£ RAW LAYER - Almacenamiento Hist√≥rico Inmutable

**Prop√≥sito**: Data hist√≥rica **congelada** para auditor√≠a y reprocesamiento.

| Caracter√≠stica | Detalle |
|----------------|---------|
| **Formato** | **Avro** (binario con esquema embebido) |
| **Esquema** | Validado y versionado |
| **Retenci√≥n** | Indefinida (base para todo an√°lisis) |
| **Particionamiento** | Por fecha (`/raw/nominas/year=2026/month=02/day=04/`) |

**¬øPor qu√© Avro?**
- ‚úÖ Soporta **evoluci√≥n de esquema** (agregar campos sin romper pipelines)
- ‚úÖ Compresi√≥n eficiente (50-70% menos que CSV)
- ‚úÖ Lectura r√°pida por herramientas Big Data (Spark, Hive)

**Proceso ETL (Staging ‚Üí Raw)**:
```python
# etl_staging_to_raw.py
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DecimalType

spark = SparkSession.builder.appName("HR-ETL").getOrCreate()

# Leer CSV de Staging
df_staging = spark.read.csv("/staging/nominas/2026-02-04/export_sap.csv", header=True)

# Validaciones b√°sicas
df_clean = df_staging.filter(df_staging.employee_id.isNotNull()) \
                     .filter(df_staging.salario_bruto > 0)

# Escribir a Raw en formato Avro particionado
df_clean.write.format("avro") \
    .partitionBy("year", "month", "day") \
    .mode("append") \
    .save("/raw/nominas/")

print(f"‚úÖ Procesados {df_clean.count()} registros desde Staging a Raw")
```

**Ejemplo de esquema Avro**:
```json
{
  "type": "record",
  "name": "Nomina",
  "namespace": "hr.payroll",
  "fields": [
    {"name": "employee_id", "type": "int"},
    {"name": "nombre", "type": "string"},
    {"name": "salario_bruto", "type": "decimal", "precision": 10, "scale": 2},
    {"name": "fecha_pago", "type": "string"},
    {"name": "departamento", "type": "string"}
  ]
}
```

---

### 3Ô∏è‚É£ MASTER LAYER - Zona Anal√≠tica

**Prop√≥sito**: Datos **listos para consumo** por BI y ML.

| Caracter√≠stica | Detalle |
|----------------|---------|
| **Formato** | **Parquet** (columnar, optimizado para queries) |
| **Esquema** | Desnormalizado con KPIs calculados |
| **Retenci√≥n** | Actualizado diariamente |
| **Particionamiento** | Por dimensi√≥n anal√≠tica (`/master/turnover_by_dept/`) |

**¬øPor qu√© Parquet?**
- ‚úÖ Almacenamiento **columnar** ‚Üí Lee solo las columnas necesarias (10x m√°s r√°pido)
- ‚úÖ Compresi√≥n superior a Avro (80% reducci√≥n vs CSV)
- ‚úÖ Compatible directo con Power BI, Pandas, SQL

**Transformaciones (Raw ‚Üí Master)**:
```python
# etl_raw_to_master.py
# Calcular KPIs agregados

from pyspark.sql.functions import col, avg, count, datediff, current_date

# Leer datos de Raw
df_nominas = spark.read.format("avro").load("/raw/nominas/")
df_asistencias = spark.read.format("avro").load("/raw/asistencias/")

# JOIN: Enriquecer con datos de asistencia
df_enriched = df_nominas.join(df_asistencias, "employee_id", "left")

# Calcular m√©tricas anal√≠ticas
df_master = df_enriched.withColumn(
    "salario_promedio_dept", avg("salario_bruto").over(Window.partitionBy("departamento"))
).withColumn(
    "dias_desde_ultima_falta", datediff(current_date(), col("fecha_ultima_falta"))
).withColumn(
    "turnover_risk_score", 
    # Modelo simple: bajo salario + muchas faltas = riesgo
    (1 - col("salario_bruto") / col("salario_promedio_dept")) * col("faltas_mes")
)

# Guardar en Master como Parquet
df_master.write.format("parquet") \
    .partitionBy("departamento") \
    .mode("overwrite") \
    .save("/master/employee_metrics/")
```

**Ejemplo de tabla Master**:
| employee_id | nombre | departamento | salario_bruto | turnover_risk_score | performance_rating |
|-------------|--------|--------------|---------------|---------------------|-------------------|
| 10234 | Juan P√©rez | Ventas | 5500.00 | 0.68 | 4.2 |
| 10567 | Mar√≠a Garc√≠a | TI | 7200.00 | 0.23 | 4.8 |

---

## ‚öôÔ∏è Flujo ETL Detallado

### Pipeline Diario (Airflow DAG)

```python
# dags/hr_daily_etl.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'people_analytics',
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

with DAG(
    'hr_daily_etl',
    default_args=default_args,
    schedule_interval='0 6 * * *',  # Diario a las 6 AM
    start_date=datetime(2026, 2, 1),
    catchup=False
) as dag:
    
    # Tarea 1: Ingestar desde SAP a Staging
    ingest_sap = PythonOperator(
        task_id='ingest_from_sap',
        python_callable=extract_sap_data
    )
    
    # Tarea 2: Validar y mover a Raw
    staging_to_raw = PythonOperator(
        task_id='staging_to_raw',
        python_callable=process_staging_to_raw
    )
    
    # Tarea 3: Calcular KPIs en Master
    raw_to_master = PythonOperator(
        task_id='raw_to_master',
        python_callable=calculate_master_metrics
    )
    
    # Dependencias
    ingest_sap >> staging_to_raw >> raw_to_master
```

### Proceso de Calidad de Datos

**Validaciones en cada capa**:

| Capa | Validaci√≥n | Acci√≥n si Falla |
|------|-----------|-----------------|
| Staging | Ninguna | Almacena todo |
| Raw | Esquema + Nulos | Rechaza registro, log de error |
| Master | Rangos l√≥gicos | Alerta a data steward |

**Ejemplo de validaci√≥n**:
```python
# Validar que salarios est√©n en rango legal peruano
df_validated = df_raw.filter(
    (col("salario_bruto") >= 1025) &  # Sueldo m√≠nimo Per√∫ 2026
    (col("salario_bruto") <= 50000)    # Tope razonable
)

# Registros rechazados
df_rejected = df_raw.subtract(df_validated)
df_rejected.write.csv("/quarantine/invalid_salaries/")
```

---

## üõ°Ô∏è Data Governance

### Principios de Calidad

**1. Completitud**
- ‚úÖ Campos obligatorios: `employee_id`, `nombre`, `departamento`
- ‚úÖ Validaci√≥n: Tasa de nulos < 2%

**2. Unicidad**
- ‚úÖ `employee_id` debe ser **√∫nico** por empleado
- ‚úÖ Detecci√≥n de duplicados en Raw Layer

**3. Consistencia**
- ‚úÖ Fechas de contrato: `fecha_ingreso < fecha_egreso`
- ‚úÖ C√≥digos de departamento seg√∫n cat√°logo maestro

**4. Precisi√≥n**
- ‚úÖ DNI: 8 d√≠gitos num√©ricos (formato peruano)
- ‚úÖ Salarios: Validaci√≥n contra planilla oficial

### Linaje de Datos (Data Lineage)

**Trazabilidad completa**:
```
Fuente Original ‚Üí Staging ‚Üí Raw ‚Üí Master ‚Üí Consumo
    SAP          ‚Üí   CSV  ‚Üí Avro ‚Üí Parquet ‚Üí Power BI

employee_id: 10234
- Origen: SAP export del 2026-02-04 10:35 AM
- Transformaciones:
  1. Conversi√≥n a Avro (2026-02-04 11:00 AM)
  2. C√°lculo turnover_risk_score (2026-02-04 12:00 PM)
- √öltima modificaci√≥n: 2026-02-04 12:15 PM
```

### Diccionario de Datos

Ver documentaci√≥n completa: [üìñ Data Dictionary](docs/data-dictionary.md)

**Campos clave**:

| Campo | Tipo | Descripci√≥n | Capa | Ejemplo |
|-------|------|-------------|------|---------|
| `employee_id` | INT | ID √∫nico del colaborador | Todas | 10234 |
| `dni` | STRING(8) | DNI peruano | Staging, Raw | "12345678" |
| `salario_bruto` | DECIMAL(10,2) | Salario mensual antes de descuentos | Raw+ | 5500.00 |
| `turnover_risk_score` | FLOAT | Score predictivo de rotaci√≥n (0-1) | Master | 0.68 |
| `performance_rating` | FLOAT | Evaluaci√≥n de desempe√±o (1-5) | Master | 4.2 |
| `departamento_cod` | STRING(3) | C√≥digo de departamento | Todas | "VEN" |

### Seguridad y Accesos

**Control de acceso por capa**:

| Rol | Staging | Raw | Master |
|-----|---------|-----|--------|
| Data Engineer | ‚úÖ RW | ‚úÖ RW | ‚úÖ RW |
| Analista RRHH | ‚ùå | ‚úÖ R | ‚úÖ R |
| BI Developer | ‚ùå | ‚ùå | ‚úÖ R |
| Gerente RRHH | ‚ùå | ‚ùå | ‚úÖ R (dashboard) |

**Datos sensibles**:
- üîí **PII** (DNI, direcci√≥n): Encriptado en Raw con AES-256
- üîí **Salarios individuales**: Solo acceso por rol aprobado
- üîí **Evaluaciones**: Anonimizadas en an√°lisis agregados

---

## üõ†Ô∏è Stack Tecnol√≥gico

### Arquitectura Cloud (Propuesta)

```mermaid
graph LR
    A[SAP API] --> B[Azure Data Factory]
    B --> C[Azure Blob Storage - Staging]
    C --> D[Azure Databricks - Spark]
    D --> E[Azure Blob Storage - Raw]
    D --> F[Azure Blob Storage - Master]
    F --> G[Power BI Service]
    F --> H[Azure ML]
```

### Componentes

| Componente | Tecnolog√≠a | Prop√≥sito |
|------------|-----------|-----------|
| **Orquestaci√≥n** | Apache Airflow / Azure Data Factory | Programar ETLs diarios |
| **Procesamiento** | PySpark / Pandas | Transformar datos |
| **Almacenamiento** | Azure Blob / AWS S3 / HDFS | Data Lake f√≠sico |
| **Cat√°logo** | Apache Atlas / AWS Glue | Metadata management |
| **BI** | Power BI / Tableau | Dashboards ejecutivos |
| **ML** | Scikit-learn / Azure ML | Modelos predictivos |

### Ejemplo de Configuraci√≥n (Azure)

```yaml
# config/azure_datalake.yaml
storage_account: hranalyticsdata
container_staging: staging
container_raw: raw
container_master: master

retention_policies:
  staging: 7_days
  raw: indefinite
  master: 365_days

encryption:
  enabled: true
  key_vault: hr-keyvault-prod
```

---

## üìä Casos de Uso

### 1. Predicci√≥n de Rotaci√≥n de Personal

**Fuentes de datos** (Master Layer):
- `employee_metrics.parquet`: Salario, performance, antig√ºedad
- `turnover_history.parquet`: Hist√≥rico de renuncias

**Modelo**:
```python
from sklearn.ensemble import RandomForestClassifier

# Features
X = master_df[['salario_bruto', 'performance_rating', 'antig√ºedad_a√±os', 
               'satisfaction_score', 'dias_ausencia']]
y = master_df['renuncio_siguiente_trimestre']  # 0 o 1

# Entrenar
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# Predecir riesgo para empleados actuales
master_df['turnover_probability'] = model.predict_proba(X)[:, 1]
```

**Impacto**: Identificar empleados en riesgo con 3 meses de anticipaci√≥n ‚Üí Intervenci√≥n proactiva.

---

### 2. Auditor√≠a de Headcount Mensual

**Query SQL sobre Master**:
```sql
-- Comparar headcount mes a mes por departamento
SELECT 
    departamento,
    COUNT(*) as headcount_actual,
    LAG(COUNT(*)) OVER (PARTITION BY departamento ORDER BY mes) as headcount_anterior,
    COUNT(*) - LAG(COUNT(*)) OVER (PARTITION BY departamento ORDER BY mes) as variacion
FROM master.employee_metrics
WHERE fecha >= '2025-01-01'
GROUP BY departamento, DATE_TRUNC('month', fecha) as mes
ORDER BY mes DESC, departamento;
```

**Dashboard**: Alerta autom√°tica si headcount > ¬±10% vs presupuesto.

---

### 3. An√°lisis de Equidad Salarial (Gender Pay Gap)

**An√°lisis estad√≠stico**:
```python
import pandas as pd
from scipy.stats import ttest_ind

df = pd.read_parquet('/master/employee_metrics/')

# Filtrar mismo puesto
df_analistas = df[df.puesto == 'Analista']

# Salarios por g√©nero
salarios_m = df_analistas[df_analistas.genero == 'M']['salario_bruto']
salarios_f = df_analistas[df_analistas.genero == 'F']['salario_bruto']

# Test estad√≠stico
t_stat, p_value = ttest_ind(salarios_m, salarios_f)

if p_value < 0.05:
    print(f"‚ö†Ô∏è Diferencia significativa detectada (p={p_value:.4f})")
    print(f"Brecha: {salarios_m.mean() - salarios_f.mean():.2f} soles")
```

**Cumplimiento**: Detecci√≥n temprana de inequidades para auditor√≠as legales.

---

### 4. Optimizaci√≥n de Time to Hire

**KPI calculado en Master**:
```python
df_master['time_to_hire_days'] = (
    df_master['fecha_contratacion'] - df_master['fecha_publicacion_vacante']
).dt.days

# Benchmark por departamento
df_benchmark = df_master.groupby('departamento').agg({
    'time_to_hire_days': ['mean', 'median', 'min', 'max']
})
```

**Hallazgo t√≠pico**: "TI tarda 45 d√≠as vs 28 d√≠as en Finanzas ‚Üí Revisar proceso de screening t√©cnico"

---

## üìö Recursos Adicionales

- [üìñ Diccionario de Datos Completo](docs/data-dictionary.md)
- [üõ°Ô∏è Framework de Calidad de Datos](docs/quality-framework.md)
- [üîê Pol√≠ticas de Governance](docs/data-governance.md)
- [üíæ Archivos de Ejemplo](examples/)

---

## üë§ Autor

**James Lalupu**  
People Analytics Specialist | Data Engineer  

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/in/jamelalupu)
[![GitHub](https://img.shields.io/badge/GitHub-Follow-black)](https://github.com/JameLalupu)

---

## üìÑ Licencia

Este es un proyecto conceptual con fines educativos y de portafolio profesional.

---

> **üí° Nota**: Este repositorio documenta una arquitectura conceptual. Los datos de ejemplo son sint√©ticos y no representan informaci√≥n real de ninguna organizaci√≥n.
