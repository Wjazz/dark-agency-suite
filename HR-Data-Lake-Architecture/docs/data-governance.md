# üõ°Ô∏è Data Governance Framework - HR Data Lake

## üéØ Objetivos del Governance

El framework de Data Governance establece pol√≠ticas y procesos para garantizar:

1. **Calidad**: Datos precisos, completos y consistentes
2. **Seguridad**: Protecci√≥n de informaci√≥n sensible (PII)
3. **Cumplimiento**: Adherencia a normativas (LGPD, GDPR equivalente)
4. **Trazabilidad**: Auditor√≠a completa del linaje de datos
5. **Accesibilidad**: Datos disponibles para usuarios autorizados

---

## üìä Dimensiones de Calidad de Datos

### 1. Completitud (Completeness)

**Definici√≥n**: % de registros con valores en campos obligatorios

**M√©tricas**:
```sql
-- % de registros con campos obligatorios completos
SELECT 
    (COUNT(*) FILTER (WHERE employee_id IS NOT NULL 
                        AND nombre_completo IS NOT NULL
                        AND departamento_cod IS NOT NULL)) * 100.0 / COUNT(*) 
    AS completeness_pct
FROM raw.employee_master;
```

**Umbrales**:
- ‚úÖ **Aceptable**: ‚â• 98%
- ‚ö†Ô∏è **Alerta**: 95-98%
- ‚ùå **Cr√≠tico**: < 95%

**Acci√≥n correctiva**: Rechazar lote de ingesta si < 95%

---

### 2. Unicidad (Uniqueness)

**Definici√≥n**: Cada entidad debe tener un identificador √∫nico sin duplicados

**Validaci√≥n**:
```sql
-- Detectar DNIs duplicados
SELECT dni, COUNT(*) as duplicados
FROM raw.employee_master
GROUP BY dni
HAVING COUNT(*) > 1;
```

**Regla**: `employee_id` y `dni` deben ser **√∫nicos** por registro

**Acci√≥n correctiva**: Cuarentena de registros duplicados ‚Üí Revisi√≥n manual

---

### 3. Consistencia (Consistency)

**Definici√≥n**: Datos coherentes entre s√≠ y alineados con reglas de negocio

**Validaciones**:

**Fechas l√≥gicas**:
```sql
-- Validar que fecha_egreso > fecha_ingreso
SELECT employee_id, fecha_ingreso, fecha_egreso
FROM raw.employment_details
WHERE fecha_egreso IS NOT NULL 
  AND fecha_egreso < fecha_ingreso;
```

**Rangos salariales**:
```sql
-- Validar salario dentro de rango legal peruano
SELECT employee_id, salario_bruto
FROM raw.compensation
WHERE salario_bruto < 1025  -- Sueldo m√≠nimo Per√∫ 2026
   OR salario_bruto > 100000;  -- Outlier sospechoso
```

**C√≥digos maestros**:
```sql
-- Validar que departamento_cod exista en cat√°logo
SELECT e.employee_id, e.departamento_cod
FROM raw.employment_details e
LEFT JOIN master.departamentos_catalogo d ON e.departamento_cod = d.codigo
WHERE d.codigo IS NULL;
```

**Acci√≥n correctiva**: Alerta autom√°tica a Data Steward

---

### 4. Precisi√≥n (Accuracy)

**Definici√≥n**: Datos reflejan la realidad correctamente

**Validaciones**:

**Formato DNI peruano**:
```python
import re

def validar_dni(dni):
    """DNI debe ser 8 d√≠gitos num√©ricos"""
    pattern = r'^\d{8}$'
    return bool(re.match(pattern, str(dni)))

# Aplicar en ETL
df_validated = df_raw[df_raw['dni'].apply(validar_dni)]
```

**Edad razonable**:
```sql
SELECT employee_id, fecha_nacimiento, 
       DATEDIFF(CURRENT_DATE, fecha_nacimiento)/365 AS edad
FROM raw.employee_master
WHERE edad < 18 OR edad > 70;
```

**Acci√≥n correctiva**: Rechazar registros inv√°lidos ‚Üí Log de errores

---

### 5. Actualidad (Timeliness)

**Definici√≥n**: Datos disponibles cuando se necesitan

**SLA del Pipeline**:
| Fuente | Frecuencia | Latencia M√°xima | Horario |
|--------|-----------|-----------------|---------|
| N√≥minas (SAP) | Mensual | 24 horas | D√≠a 1 del mes |
| Asistencias (ADP) | Diaria | 6 horas | 6 AM |
| Evaluaciones | Semestral | 48 horas | Fin de per√≠odo |

**Monitoreo**:
```python
# Airflow DAG con alertas
if (datetime.now() - last_update_time).hours > SLA_HOURS:
    send_alert_to_slack(f"‚ö†Ô∏è Data delayed: {source_name}")
```

---

## üîê Seguridad y Privacidad

### Clasificaci√≥n de Datos

| Nivel | Descripci√≥n | Ejemplos | Protecci√≥n |
|-------|-------------|----------|-----------|
| **P√∫blico** | Sin riesgo si se expone | Nombre departamento | Ninguna |
| **Interno** | Solo empleados | M√©tricas agregadas | Autenticaci√≥n |
| **Confidencial** | Acceso restringido | Salario individual | Roles espec√≠ficos |
| **Cr√≠tico (PII)** | Datos personales | DNI, direcci√≥n | Encriptaci√≥n |

### Encriptaci√≥n de PII

**Campos sensibles**:
- `dni`
- `telefono_movil`
- `direccion_domicilio`
- `cuenta_bancaria`

**Implementaci√≥n**:
```python
from cryptography.fernet import Fernet

# Generar clave (almacenar en Azure Key Vault)
key = Fernet.generate_key()
cipher = Fernet(key)

# Encriptar al escribir en Raw Layer
df_raw['dni_encrypted'] = df_raw['dni'].apply(
    lambda x: cipher.encrypt(str(x).encode())
)

# Guardar solo versi√≥n encriptada
df_raw.drop(columns=['dni']).write.parquet('/raw/employee_master/')
```

**Desencriptaci√≥n**: Solo usuarios con rol `HR_ADMIN`

---

### Control de Acceso (RBAC)

**Roles definidos**:

| Rol | Staging | Raw | Master | Descripci√≥n |
|-----|---------|-----|--------|-------------|
| `DATA_ENGINEER` | RW | RW | RW | Control total del pipeline |
| `HR_ANALYST` | - | R | R | An√°lisis de datos anonimizados |
| `HR_ADMIN` | - | R (incl. PII) | R (incl. PII) | Acceso a datos sensibles |
| `BI_DEVELOPER` | - | - | R | Solo capa Master para dashboards |
| `MANAGER` | - | - | R (filtered) | Solo su departamento |

**Implementaci√≥n (Azure)**:
```json
{
  "role": "HR_ANALYST",
  "permissions": [
    {
      "path": "/raw/*",
      "actions": ["read"],
      "conditions": {
        "exclude_columns": ["dni_encrypted", "cuenta_bancaria"]
      }
    }
  ]
}
```

---

## üìã Linaje de Datos (Data Lineage)

### Trazabilidad End-to-End

**Objetivo**: Responder "¬øDe d√≥nde viene este dato?"

**Herramienta**: Apache Atlas / AWS Glue Data Catalog

**Ejemplo de linaje**:
```
Campo: turnover_risk_score (Master Layer)

Origen:
‚îî‚îÄ master.employee_metrics.turnover_risk_score
   ‚îî‚îÄ Calculado por: etl_raw_to_master.py (l√≠nea 45)
      ‚îî‚îÄ Inputs:
         ‚îú‚îÄ raw.compensation.salario_bruto
         ‚îÇ  ‚îî‚îÄ staging/nominas/2026-02-04/export_sap.csv (columna: SALARIO_BRUTO)
         ‚îÇ     ‚îî‚îÄ SAP SuccessFactors (API export 2026-02-04 10:35 AM)
         ‚îÇ
         ‚îú‚îÄ raw.engagement.satisfaction_score
         ‚îÇ  ‚îî‚îÄ staging/encuestas/2025-12-15/clima_laboral.json (campo: satisfaction)
         ‚îÇ     ‚îî‚îÄ Sistema encuestas internas (aplicado 2025-12-15)
         ‚îÇ
         ‚îî‚îÄ raw.attendance.faltas_mes
            ‚îî‚îÄ staging/asistencias/2026-02-04/marcaciones_adp.csv (agregado)
               ‚îî‚îÄ ADP Workforce Now (daily sync)

√öltima modificaci√≥n: 2026-02-04 12:15 PM (job_id: etl_20260204_001)
```

---

### Versionado de Transformaciones

**Control de versiones del c√≥digo ETL**:
```bash
# Git commit obligatorio antes de ejecutar
git log --oneline etl_raw_to_master.py

a1b2c3d (2026-02-04) Agregado c√°lculo de turnover_risk_score
d4e5f6g (2026-01-15) Fix: filtro de empleados activos
```

**Metadatos en Master Layer**:
```python
# Agregar columnas de auditor√≠a
df_master['_created_at'] = current_timestamp()
df_master['_etl_version'] = git_commit_hash
df_master['_source_file'] = input_file_name()
```

---

## üìê Est√°ndares de Datos

### Convenciones de Nomenclatura

**Tablas**:
- Singular, snake_case: `employee_master`, `compensation`
- Prefijo por capa: `staging_`, `raw_`, `master_`

**Columnas**:
- snake_case: `employee_id`, `fecha_ingreso`
- Sufijos:
  - `_id`: Identificadores
  - `_cod`: C√≥digos categ√≥ricos
  - `_pct`: Porcentajes
  - `_score`: Scores calculados

**Fechas**:
- Formato: `YYYY-MM-DD` (ISO 8601)
- Nombres: `fecha_[evento]` (ej: `fecha_contratacion`)

**Moneda**:
- Siempre incluir columna `moneda` (PEN, USD)
- Decimales: 2 d√≠gitos (ej: `5500.00`)

---

## üîç Monitoreo y Alertas

### Dashboard de Calidad de Datos

**M√©tricas en tiempo real** (Power BI):

| M√©trica | Umbral | Valor Actual | Estado |
|---------|--------|--------------|--------|
| Completeness | ‚â•98% | 99.2% | ‚úÖ |
| Registros duplicados | 0 | 0 | ‚úÖ |
| Latencia pipeline (hrs) | ‚â§6 | 4.5 | ‚úÖ |
| Errores ETL (√∫ltima corrida) | 0 | 3 | ‚ö†Ô∏è |

### Alertas Autom√°ticas

**Configuraci√≥n (Airflow)**:
```python
def data_quality_check(**context):
    df = spark.read.parquet('/raw/employee_master/')
    
    # Check 1: Completeness
    completeness = df.filter(df.employee_id.isNotNull()).count() / df.count()
    if completeness < 0.98:
        raise AirflowException(f"‚ùå Completeness {completeness:.2%} < 98%")
    
    # Check 2: Duplicates
    duplicates = df.groupBy('employee_id').count().filter('count > 1').count()
    if duplicates > 0:
        raise AirflowException(f"‚ùå {duplicates} duplicate employee_ids found")
    
    # Check 3: Invalid salaries
    invalid_salaries = df.filter((df.salario_bruto < 1025) | (df.salario_bruto > 100000)).count()
    if invalid_salaries > 0:
        send_slack_alert(f"‚ö†Ô∏è {invalid_salaries} salaries out of range")

# DAG task
quality_check = PythonOperator(
    task_id='data_quality_check',
    python_callable=data_quality_check,
    trigger_rule='all_success'
)
```

**Canales de notificaci√≥n**:
- üî¥ Cr√≠tico ‚Üí Email a Data Engineer + Jira ticket
- üü° Alerta ‚Üí Slack #data-quality
- üü¢ Info ‚Üí Log interno

---

## üë• Roles y Responsabilidades

| Rol | Responsable | Responsabilidades |
|-----|------------|------------------|
| **Data Owner** | Gerente de RRHH | Aprueba pol√≠ticas de acceso y uso de datos |
| **Data Steward** | Analista Senior RRHH | Valida calidad, resuelve inconsistencias |
| **Data Engineer** | Equipo TI | Implementa pipelines, mantiene infraestructura |
| **Data Custodian** | Administrador de Sistemas | Gestiona backups, seguridad f√≠sica |
| **Data Consumer** | Analistas/Gerentes | Consume datos para an√°lisis y decisiones |

---

## üìÖ Ciclo de Revisi√≥n

**Periodicidad de auditor√≠as**:

| Actividad | Frecuencia | Responsable |
|-----------|-----------|-------------|
| Revisi√≥n de calidad de datos | Diaria | Data Engineer |
| Actualizaci√≥n de diccionario | Trimestral | Data Steward |
| Auditor√≠a de accesos | Mensual | InfoSec |
| Certificaci√≥n de cumplimiento | Anual | Legal + RRHH |

---

## üìÑ Cumplimiento Normativo

### Legislaci√≥n Aplicable (Per√∫)

**Ley de Protecci√≥n de Datos Personales (Ley N¬∞ 29733)**:
- ‚úÖ Consentimiento para uso de datos personales
- ‚úÖ Derecho al olvido (eliminar datos de empleados que lo soliciten)
- ‚úÖ Retenci√≥n m√°xima: 5 a√±os post-egreso

**Implementaci√≥n**:
```python
# Script de anonimizaci√≥n para ex-empleados (> 5 a√±os)
def anonymize_old_records():
    cutoff_date = datetime.now() - timedelta(days=5*365)
    
    df = spark.read.parquet('/raw/employee_master/')
    df_anonymized = df.filter(df.fecha_egreso < cutoff_date) \
        .withColumn('nombre_completo', lit('ANONIMIZADO')) \
        .withColumn('dni', lit('00000000')) \
        .withColumn('email_corporativo', lit('redacted@empresa.com'))
    
    df_anonymized.write.mode('overwrite').parquet('/raw/employee_master_anon/')
```

---

## üîó Referencias

- [ISO 8000: Data Quality](https://www.iso.org/standard/50798.html)
- [DAMA-DMBOK: Data Governance](https://www.dama.org/cpages/body-of-knowledge)
- [Ley N¬∞ 29733 (Protecci√≥n de Datos Personales - Per√∫)](https://www.gob.pe/institucion/minjus/normas-legales/241865-29733)

---

**Versi√≥n**: 1.0  
**Autor**: James Lalupu  
**√öltima actualizaci√≥n**: 2026-02-04
